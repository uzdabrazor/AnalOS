---
title: "Setting up Ollama"
description: "Configure Ollama to work with AnalOS for local AI capabilities"
icon: "robot"
---

## Quick Setup

<Steps>
  <Step title="Navigate to AI Settings">
    Navigate to `chrome://analos/settings` or click the **Settings** icon on the new tab page to add Ollama as a provider.
  </Step>
  <Step title="Get Model ID">
    Get the model ID of your Ollama model (e.g., `gpt-oss:20b`)
  </Step>
  <Step title="Start Ollama Server">
    Start Ollama:

    ```bash
    ollama serve
    ```
  </Step>
  <Step title="Select and Use">
    Select the model in agent and start using it! ðŸ¥³
  </Step>
</Steps>


## Detailed Visual Guide

### Step 1: Navigate to Settings Page

Navigate to `chrome://analos/settings` or click the **Settings** icon on the new tab page

<Frame>
  ![Navigate to settings page](/images/setting-up-ollama/ollama-step1.png)
</Frame>

### Step 2: Get the Ollama Model ID

Identify and copy your Ollama model ID for configuration.

<Frame>
  ![Get the ollama model ID](/images/setting-up-ollama/ollama-step2.png)
</Frame>

### Step 3: Start Ollama from CLI

Run Ollama to start the local server:

```bash
ollama serve
```

<Frame>
  ![Start Ollama from CLI](/images/setting-up-ollama/ollama-step3.png)
</Frame>

### Step 4: Use the Model

Select the model in the Agent dropdown and start using it! ðŸš€

<Frame>
  ![Use the model](/images/setting-up-ollama/ollama-step4.png)
</Frame>

## Alternative: LM Studio

<Card title="LM Studio Setup" icon="desktop" href="/local-LLMs/lm-studio">
  If you prefer not to run Ollama from the command line, LM Studio provides a more user-friendly alternative with a graphical interface.
</Card>